# Machine Learning Reading
I like to read machine learning blogs and literatures. This page contains links to ones that I enjoyed reading.   

## AI for Everyone
* [Interactive introduction to ML](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)
* [Visualizing Neural Networks](https://github.com/tensorspace-team/tensorspace)
* [Interactive tutorials](https://github.com/stared/interactive-machine-learning-list/blob/master/websites.yaml)
* [Build interactive ML app](https://github.com/streamlit/streamlit/tree/develop/examples)

## Differential calculus in machine learning
* [Physics guided NN](https://towardsdatascience.com/physics-guided-neural-networks-pgnns-8fe9dbad9414)
* [Ordinary differential equation and RESNET](https://www.youtube.com/watch?v=sAvtPr1IGB0&t=379s)
* [Physics informed Deep learning](https://arxiv.org/abs/1711.10561)

## ML cheat sheets
* [Mathematics](https://avantlive.wordpress.com/2019/04/29/fathoming-the-deep-in-deep-learning-a-practical-approach/)

## Supervised learning
* [Multi-task in Earth Quake Prediction](https://www.kaggle.com/dkaraflos/1-geomean-nn-and-6featlgbm-2-259-private-lb)
* [Multi-task learning](https://medium.com/huggingface/beating-the-state-of-the-art-in-nlp-with-hmtl-b4e1d5c3faf) 
* [MTL on various tasks](https://towardsdatascience.com/multitask-learning-teach-your-ai-more-to-make-it-better-dde116c2cd40)

## Graph-based Machine learning
* [Dynamic Graph Embedding](https://github.com/woojeongjin/dynamic-KG)
* [The Unreasonable Effectiveness of Structure](https://www.youtube.com/watch?v=t4k5LKCpboc)
* [Object2Vec embedding using SageMaker](https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/)
* [GraphSage](http://i.stanford.edu/~jure/pub/talks2/graphsage_gin-ita-feb19.pdf)
* [Graph learning in Tensorflow](https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora)

## Pipleline and DevOps
* [End-to-End ML pipeline using pachyderm and kubeflow](https://www.youtube.com/watch?v=ZEGdSLWdrH0)

## Privacy in Machine learning
* [PySyft and Federated learning](https://www.youtube.com/watch?v=kAiAo_MCw20)
* [Federated learning comics](https://federated.withgoogle.com/#top)
* [Tensorflow Privacy](https://medium.com/tensorflow/introducing-tensorflow-privacy-learning-with-differential-privacy-for-training-data-b143c5e801b6)

## AutoML and Neural Architecture Search
* [EfficientNet](https://arxiv.org/pdf/1905.11946v2.pdf)
* [ProxylessNAS](https://arxiv.org/abs/1812.00332)
* [Lottery Ticket Hypothesis](https://openreview.net/pdf?id=rJl-b3RcF7)  
  *[Targeted dropout](https://for.ai/blog/targeted-dropout/)
* [Learning rate finder](https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/)

## Language model and Attention
* [BERT from scratch](http://www.peterbloem.nl/blog/transformers?utm_content=buffer2a7ae&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer)
* [Breaking down BERT](https://towardsdatascience.com/breaking-bert-down-430461f60efb)
* [MASS](https://www.microsoft.com/en-us/research/blog/introducing-mass-a-pre-training-method-that-outperforms-bert-and-gpt-in-sequence-to-sequence-language-generation-tasks/)
* [Transformer application using Tensorflow](https://www.tensorflow.org/alpha/tutorials/sequences/transformer)
* [BERT](https://jalammar.github.io/illustrated-bert/)
* [How to apply BERT](https://www.youtube.com/watch?v=bDxFvr1gpSU)
* [Transformer](https://towardsdatascience.com/transformers-141e32e69591)

## Applied NLP and its Pipeline
* [NLP and attention to code search](https://ai.facebook.com/blog/neural-code-search-ml-based-code-search-using-natural-language-queries/)
* [Parsing and Entity extraction](https://www.kaggle.com/shivamb/1-bulletin-structuring-engine-cola)
* [word2vec in Windows defender](https://www.microsoft.com/security/blog/2019/09/03/deep-learning-rises-new-methods-for-detecting-malicious-powershell/)  

## Recommender Systems
* [RecSys using Transformer architecture](https://aisc.ai.science/blog/2019/generating-high-fidelity-images)
* [Movie Audience Prediction using Deep learning](https://cloud.google.com/blog/products/ai-machine-learning/how-20th-century-fox-uses-ml-to-predict-a-movie-audience)

## Dimension reduction
* [10 tips for dimension reduction](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006907)

## Time series
* [Stock price prediction using LSTM](https://www.altumintelligence.com/articles/a/Time-Series-Prediction-Using-LSTM-Deep-Neural-Networks)
* [Forecasting at Uber](https://eng.uber.com/m4-forecasting-competition/)
* [N-Beats](https://mc.ai/n-beats%E2%80%8A-%E2%80%8Abeating-statistical-models-with-neural-nets/)

## Anomaly Detection
* [Isolation forest](https://www.youtube.com/watch?v=RyFQXQf4w4w)
* [Probabilistic anomaly detection in Time Series](https://www.microsoft.com/security/blog/2019/12/18/data-science-for-cybersecurity-a-probabilistic-time-series-model-for-detecting-rdp-inbound-brute-force-attacks/)

## Model diagnosis
* [Uber Manifold][Blog](https://eng.uber.com/manifold/)[Code](https://github.com/uber/manifold)

## Interpretation of ML Model Output
* [Interpretable ML Book](https://christophm.github.io/interpretable-ml-book/rulefit.html)
* [Monotonic Gradient Boosting Machine](https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html)
* [Monotonic Logistic Regression](https://www.microsoft.com/security/blog/2019/07/25/new-machine-learning-model-sifts-through-the-good-to-unearth-the-bad-in-evasive-malware/)
* [DeepLIFT](https://github.com/kundajelab/deeplift)
* [PDP and LIME](https://towardsdatascience.com/interpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2)
* [Anchors and LIME](https://towardsdatascience.com/anchor-your-model-interpretation-by-anchors-aa4ed7104032)
* Shapley
* [Collection of Interpretability tutorials](https://github.com/jphall663/awesome-machine-learning-interpretability)

## Bayesian methods
* [Uncertainty and confidence interval](https://erikbern.com/2018/10/08/the-hackers-guide-to-uncertainty-estimates.html)
* [Bayesian optimization](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/)
* [Gaussian processes with example](https://planspace.org/20181226-gaussian_processes_are_not_so_fancy/)
* [Gaussian Process in Numpy](http://krasserm.github.io/2018/03/19/gaussian-processes/)
* [Visual exploration of GP](https://distill.pub/2019/visual-exploration-gaussian-processes/)

## Neural network architecture and its applications
* [ML using Electronic Health Records](https://goku.me/blog/EHR)

## Neural network training
* [A recipe for training](http://karpathy.github.io/2019/04/25/recipe/)
* [Vanishing Gradients](https://towardsdatascience.com/avoiding-the-vanishing-gradients-problem-96183fd03343)

## Generative Adversarial Network (GAN)
* [Unusual Autoencoders](https://colinraffel.com/talks/vector2018few.pdf)
* [WAE](https://github.com/MonirZaman/ICLR-Conference-Insights#wasserstein-auto-encoders-wae)
* [BigGAN playground](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb)

## Kaggle Boiler plate
* [Time series prediction - Earth quake dataset](https://www.kaggle.com/dkaraflos/1-geomean-nn-and-6featlgbm-2-259-private-lb)
* [2](https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough)
* [3](https://towardsdatascience.com/how-to-begin-competing-on-kaggle-bd9b5f32dbbc)



