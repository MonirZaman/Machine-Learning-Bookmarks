# Machine Learning Reading
I am an avid reader of machine learning blogs and literatures. This page contains links to ones that I enjoyed reading.   

## ML cheat sheets
* [Mathematics](https://avantlive.wordpress.com/2019/04/29/fathoming-the-deep-in-deep-learning-a-practical-approach/)

## Supervised learning
* [Multi-task in Earth Quake Prediction](https://www.kaggle.com/dkaraflos/1-geomean-nn-and-6featlgbm-2-259-private-lb)
* [Multi-task learning](https://medium.com/huggingface/beating-the-state-of-the-art-in-nlp-with-hmtl-b4e1d5c3faf) 
* [MTL on various tasks](https://towardsdatascience.com/multitask-learning-teach-your-ai-more-to-make-it-better-dde116c2cd40)

## Graph-based Machine learning
* [Dynamic Graph Embedding](https://github.com/woojeongjin/dynamic-KG)
* [The Unreasonable Effectiveness of Structure](https://www.youtube.com/watch?v=t4k5LKCpboc)
* [Object2Vec embedding using SageMaker](https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/)
* [GraphSage](http://i.stanford.edu/~jure/pub/talks2/graphsage_gin-ita-feb19.pdf)

## Privacy in Machine learning
* [PySyft and Federated learning](https://www.youtube.com/watch?v=kAiAo_MCw20)
* [Tensorflow Privacy](https://medium.com/tensorflow/introducing-tensorflow-privacy-learning-with-differential-privacy-for-training-data-b143c5e801b6)

## AutoML and Neural Architecture Search
* [EfficientNet](https://arxiv.org/pdf/1905.11946v2.pdf)
* [ProxylessNAS](https://arxiv.org/abs/1812.00332)
* [Lottery Ticket Hypothesis](https://openreview.net/pdf?id=rJl-b3RcF7)  
  *[Targeted dropout](https://for.ai/blog/targeted-dropout/)
* [Learning rate finder](https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/)

## Language model and Attention
* [BERT from scratch](http://www.peterbloem.nl/blog/transformers?utm_content=buffer2a7ae&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer)
* [Breaking down BERT](https://towardsdatascience.com/breaking-bert-down-430461f60efb)
* [MASS](https://www.microsoft.com/en-us/research/blog/introducing-mass-a-pre-training-method-that-outperforms-bert-and-gpt-in-sequence-to-sequence-language-generation-tasks/)
* [Transformer application using Tensorflow](https://www.tensorflow.org/alpha/tutorials/sequences/transformer)
* [BERT](https://jalammar.github.io/illustrated-bert/)
* [How to apply BERT](https://www.youtube.com/watch?v=bDxFvr1gpSU)
* [Transformer](https://towardsdatascience.com/transformers-141e32e69591)

## NLP Pipeline
* [Parsing and Entity extraction](https://www.kaggle.com/shivamb/1-bulletin-structuring-engine-cola)

## Recommender Systems
* [RecSys using Transformer architecture](https://aisc.ai.science/blog/2019/generating-high-fidelity-images)

## Dimension reduction
* [10 tips for dimension reduction](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006907)

## Time series
* [Stock price prediction using LSTM](https://www.altumintelligence.com/articles/a/Time-Series-Prediction-Using-LSTM-Deep-Neural-Networks)
* [Forecasting at Uber](https://eng.uber.com/m4-forecasting-competition/)

## Anomaly Detection
* [Isolation forest](https://www.youtube.com/watch?v=RyFQXQf4w4w)

## Model diagnosis
* [Uber Manifold](https://eng.uber.com/manifold/)

## Interpretation of ML Model Output
* [Interpretable ML Book](https://christophm.github.io/interpretable-ml-book/rulefit.html)
* [Monotonic Gradient Boosting Machine](https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html)
* [Monotonic Logistic Regression](https://www.microsoft.com/security/blog/2019/07/25/new-machine-learning-model-sifts-through-the-good-to-unearth-the-bad-in-evasive-malware/)
* [DeepLIFT](https://github.com/kundajelab/deeplift)
* [PDP and LIME](https://towardsdatascience.com/interpretable-ai-or-how-i-learned-to-stop-worrying-and-trust-ai-e61f9e8ee2c2)
* [Anchors and LIME](https://towardsdatascience.com/anchor-your-model-interpretation-by-anchors-aa4ed7104032)
* Shapley
* [Collection of Interpretability tutorials](https://github.com/jphall663/awesome-machine-learning-interpretability)

## Bayesian methods
* [Uncertainty and confidence interval](https://erikbern.com/2018/10/08/the-hackers-guide-to-uncertainty-estimates.html)

## Neural network architecture and its applications
* [ML using Electronic Health Records](https://goku.me/blog/EHR)

## Neural network training
* [A recipe for training](http://karpathy.github.io/2019/04/25/recipe/)

## Generative Adversarial Network (GAN)
* [Unusual Autoencoders](https://colinraffel.com/talks/vector2018few.pdf)
* [WAE](https://github.com/MonirZaman/ICLR-Conference-Insights#wasserstein-auto-encoders-wae)
* [BigGAN playground](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb)

## Kaggle Boiler plate
* [Time series prediction - Earth quake dataset](https://www.kaggle.com/dkaraflos/1-geomean-nn-and-6featlgbm-2-259-private-lb)
* [2](https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough)
* [3](https://towardsdatascience.com/how-to-begin-competing-on-kaggle-bd9b5f32dbbc)



